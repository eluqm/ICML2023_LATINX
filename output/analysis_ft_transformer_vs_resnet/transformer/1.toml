coefs = [ 0, 0.25, 0.5, 0.75, 1 ]
mlp_bias = false
mlp_d_layer = 256
mlp_n_layers = 3
model = 'transformer'
n_features = 100
n_informative = 50
noise_std = 0
seed = 1
tree_max_depth = 10
tree_n_nodes = 100
tree_n_trees = 30

[dataset_size]
test = 100000
train = 500000
val = 50000

[model_config]
activation = 'reglu'
attention_dropout = 0.2
d_ffn_factor = 1.333333333333333
d_token = 192
ffn_dropout = 0.1
initialization = 'kaiming'
n_heads = 8
n_layers = 3
prenormalization = true
residual_dropout = 0.0
token_bias = true

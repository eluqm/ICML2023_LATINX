{
    "config": {
        "data": {
            "path": "data/higgs_small",
            "y_policy": "mean_std"
        },
        "fit": {
            "early_stopping_rounds": 50,
            "verbose": true
        },
        "model": {
            "colsample_bytree": 0.7999440674588046,
            "learning_rate": 0.04183849804413404,
            "min_child_samples": 81,
            "min_child_weight": 0.00676057460469568,
            "n_estimators": 2000,
            "n_jobs": 1,
            "num_leaves": 94,
            "reg_lambda": 0.006637134146517112,
            "subsample": 0.6165566522563218
        },
        "seed": 4
    },
    "environment": {},
    "dataset": "higgs_small",
    "algorithm": "lightgbm_",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.8256697733603879,
                "recall": 0.8115746061794334,
                "f1-score": 0.8185615165618234,
                "support": 29582
            },
            "1": {
                "precision": 0.8344766146993319,
                "recall": 0.8471811878203196,
                "f1-score": 0.8407809110629068,
                "support": 33170
            },
            "accuracy": 0.8303958439571647,
            "macro avg": {
                "precision": 0.8300731940298599,
                "recall": 0.8293778969998765,
                "f1-score": 0.8296712138123651,
                "support": 62752
            },
            "weighted avg": {
                "precision": 0.8303249704411626,
                "recall": 0.8303958439571647,
                "f1-score": 0.8303064380878454,
                "support": 62752
            },
            "roc_auc": 0.9125345567087124,
            "score": 0.8303958439571647
        },
        "val": {
            "0": {
                "precision": 0.7118621064060804,
                "recall": 0.7091671173607356,
                "f1-score": 0.7105120563532917,
                "support": 7396
            },
            "1": {
                "precision": 0.7414663461538461,
                "recall": 0.7439700916546068,
                "f1-score": 0.7427161088369852,
                "support": 8292
            },
            "accuracy": 0.7275624681285059,
            "macro avg": {
                "precision": 0.7266642262799632,
                "recall": 0.7265686045076711,
                "f1-score": 0.7266140825951385,
                "support": 15688
            },
            "weighted avg": {
                "precision": 0.7275096303727092,
                "recall": 0.7275624681285059,
                "f1-score": 0.7275337291729492,
                "support": 15688
            },
            "roc_auc": 0.8087224042826242,
            "score": 0.7275624681285059
        },
        "test": {
            "0": {
                "precision": 0.7110843900306077,
                "recall": 0.7036235803136831,
                "f1-score": 0.707334311966509,
                "support": 9245
            },
            "1": {
                "precision": 0.7380997897151597,
                "recall": 0.7450072358900145,
                "f1-score": 0.7415374273779229,
                "support": 10365
            },
            "accuracy": 0.7254971953085161,
            "macro avg": {
                "precision": 0.7245920898728837,
                "recall": 0.7243154081018488,
                "f1-score": 0.724435869672216,
                "support": 19610
            },
            "weighted avg": {
                "precision": 0.7253635648256297,
                "recall": 0.7254971953085161,
                "f1-score": 0.725412603207677,
                "support": 19610
            },
            "roc_auc": 0.8044718243808925,
            "score": 0.7254971953085161
        }
    },
    "time": "0:00:08"
}
